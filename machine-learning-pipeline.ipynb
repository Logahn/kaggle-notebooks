{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1020485,"sourceType":"datasetVersion","datasetId":545987}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-28T11:20:11.750329Z","iopub.execute_input":"2024-05-28T11:20:11.751343Z","iopub.status.idle":"2024-05-28T11:20:12.135722Z","shell.execute_reply.started":"2024-05-28T11:20:11.751305Z","shell.execute_reply":"2024-05-28T11:20:12.134647Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"/kaggle/input/housing/target.csv\n/kaggle/input/housing/train.csv\n/kaggle/input/housing/test.csv\n/kaggle/input/housing/AmesHousing.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"# connecting to file path\niowa_file_path = '../input/housing/AmesHousing.csv'\nhome_data = pd.read_csv(iowa_file_path)\nhome_data.head()\nhome_data.columns\n\n# data cleaning\nhome_data.dropna(axis=0)\nhome_data.describe()\n\n#select the target column\ny = home_data.SalePrice\n\n#select the features for the model\nfeatures = ['Lot Area','Year Built','1st Flr SF','2nd Flr SF','Full Bath','Bedroom AbvGr','TotRms AbvGrd']\nX = home_data[features]\nX.head()\n\n#using Decision Tree Regressor\nfrom sklearn.tree import DecisionTreeRegressor\nhousing_model = DecisionTreeRegressor(random_state = 1)\nhousing_model.fit(X,y)\n\n# print out the predictions\npredictions = housing_model.predict(X)\nprint(predictions)\n\n#Model Validation\nfrom sklearn.metrics import mean_absolute_error\nmean_absolute_error(y, predictions)\n\nfrom sklearn.model_selection import train_test_split\ntrain_X, val_X, train_y, val_y = train_test_split(X,y,random_state=0)\nhousing_model = DecisionTreeRegressor()\nhousing_model.fit(train_X, train_y)\n\nval_predictions = housing_model.predict(val_X)\nprint(mean_absolute_error(val_y, val_predictions))\n\n#Underfitting and Overfitting\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.tree import DecisionTreeRegressor\n\ndef get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y):\n    model = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes, random_state=0)\n    model.fit(train_X, train_y)\n    preds_val = model.predict(val_X)\n    mae = mean_absolute_error(val_y, preds_val)\n    return(mae)\n# compare MAE with differing values of max_leaf_nodes\nfor max_leaf_nodes in [5, 50, 500, 5000]:\n    my_mae = get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y)\n    print(\"Max leaf nodes: %d  \\t\\t Mean Absolute Error:  %d\" %(max_leaf_nodes, my_mae))","metadata":{"_kg_hide-output":false,"execution":{"iopub.status.busy":"2024-05-28T11:20:16.250006Z","iopub.execute_input":"2024-05-28T11:20:16.251039Z","iopub.status.idle":"2024-05-28T11:20:17.050569Z","shell.execute_reply.started":"2024-05-28T11:20:16.251003Z","shell.execute_reply":"2024-05-28T11:20:17.049433Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"[215000. 105000. 172000. ... 132000. 170000. 188000.]\n28456.099590723057\nMax leaf nodes: 5  \t\t Mean Absolute Error:  34138\nMax leaf nodes: 50  \t\t Mean Absolute Error:  24989\nMax leaf nodes: 500  \t\t Mean Absolute Error:  26177\nMax leaf nodes: 5000  \t\t Mean Absolute Error:  27635\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\n\nmodel = RandomForestRegressor(random_state = 0)\nmodel.fit(train_X,train_y)\nval_prediction = model.predict(val_X)\nprint(mean_absolute_error(val_y, val_prediction))","metadata":{"execution":{"iopub.status.busy":"2024-05-28T11:50:31.637954Z","iopub.execute_input":"2024-05-28T11:50:31.638374Z","iopub.status.idle":"2024-05-28T11:50:32.502125Z","shell.execute_reply.started":"2024-05-28T11:50:31.638346Z","shell.execute_reply":"2024-05-28T11:50:32.500986Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"20336.282925109605\n","output_type":"stream"}]},{"cell_type":"code","source":"# Run the code to save predictions in the format used for competition scoring\n\noutput = pd.DataFrame({'Id': test_data.Id,\n                       'SalePrice': test_preds})\noutput.to_csv('submission.csv', index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Intermediate Level","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntrain_data_path = \"../input/train.csv\"\ntest_data_path = \"../input/test.csv\"\n\ntrain_data = pd.read_csv(train_data_path, index_col='Id')\ntest_data = pd.read_csv(test_data_path, index_col='Id')\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y = train_data.SalePrice\n\nfeatures = ['LotArea', 'YearBuilt', '1stFlrSF', '2ndFlrSF', 'FullBath', 'BedroomAbvGr', 'TotRmsAbvGrd']\nX = train_data[features]\n# X_test = test_data[features]\n\nX_train, X_val, y_train, y_val = train_test_split(X,y,test_size = 0.2)\n\nfrom sklearn.ensemble import RandomForestRegressor\nmodel = RandomForestRegressor(random_state = 1)\nmodel.fit(X_train, y_train)\nprediction = model.predict(X_val)\nmodel.score(y_val,prediction)\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Missing Values ","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.impute import SimpleImputer\n\n# Imputation\nmy_imputer = SimpleImputer()\nimputed_X_train = pd.DataFrame(my_imputer.fit_transform(X_train))\nimputed_X_valid = pd.DataFrame(my_imputer.transform(X_valid))\n\n# Imputation removed column names; put them back\nimputed_X_train.columns = X_train.columns\nimputed_X_valid.columns = X_valid.columns\n\nprint(\"MAE from Approach 2 (Imputation):\")\nprint(score_dataset(imputed_X_train, imputed_X_valid, y_train, y_valid))\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# Load the data\ndata = pd.read_csv('../input/melbourne-housing-snapshot/melb_data.csv')\n\n# Select target\ny = data.Price\n\n# To keep things simple, we'll use only numerical predictors\nmelb_predictors = data.drop(['Price'], axis=1)\nX = melb_predictors.select_dtypes(exclude=['object'])\n\n# Divide data into training and validation subsets\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,\n                                                      random_state=0)\n# Function for comparing different approaches\ndef score_dataset(X_train, X_valid, y_train, y_valid):\n    model = RandomForestRegressor(n_estimators=10, random_state=0)\n    model.fit(X_train, y_train)\n    preds = model.predict(X_valid)\n    return mean_absolute_error(y_valid, preds)\n\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.impute import SimpleImputer\n\n# Imputation\nmy_imputer = SimpleImputer()\nimputed_X_train = pd.DataFrame(my_imputer.fit_transform(X_train))\nimputed_X_valid = pd.DataFrame(my_imputer.transform(X_valid))\n\n# Imputation removed column names; put them back\nimputed_X_train.columns = X_train.columns\nimputed_X_valid.columns = X_valid.columns\n\nprint(\"MAE from Approach 2 (Imputation):\")\nprint(score_dataset(imputed_X_train, imputed_X_valid, y_train, y_valid))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\n\ndata = pd.read_csv('../input/melbourne-housing-snapshot/melb_data.csv')\n\n#split dataset\ny = data.Price\nx = data.drop(['Price'], axis=1)\n\nX_train, X_valid, y_train, y_valid = train_test_split(x, y, train_size = 0.8, test_size = 0.2, random_state = 0)\n\ncols_with_missing = [col for col in X_train.columns if X_train[col].isnull().any()]\n\nX_train.drop(cols_with_missing, axis=1, inplace=True)\nX_valid.drop(cols_with_missing, axis=1, inplace=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check for columns with low cardinality\nlow_cardinality_cols = [cname for cname in X_train_full.columns if X_train_full[cname].nunique() < 10 and \n                        X_train_full[cname].dtype == \"object\"]\n\n# Select numerical columns\nnumerical_cols = [cname for cname in X_train_full.columns if X_train_full[cname].dtype in ['int64', 'float64']]\n\n# Keep selected columns only\nmy_cols = low_cardinality_cols + numerical_cols\nX_train = X_train_full[my_cols].copy()\nX_valid = X_valid_full[my_cols].copy()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fetching list of categorical variables\ns = (X.train.dtypes == 'object')\nobject_cols = list(s[s].index)\n\nprint(object_cols)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dropping Categorical Variables","metadata":{}},{"cell_type":"code","source":"drop_X_train = X_train.select_dtypes(exclude=['object'])\ndrop_X_valid = X_valid.select_dtypes(exclude=['object'])\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Ordinal Encoding","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import OrdinalEncoder\n\n# Make copy to avoid changing original data \nlabel_X_train = X_train.copy()\nlabel_X_valid = X_valid.copy()\n\n# Apply ordinal encoder to each column with categorical data\nordinal_encoder = OrdinalEncoder()\nlabel_X_train[object_cols] = ordinal_encoder.fit_transform(X_train[object_cols])\nlabel_X_valid[object_cols] = ordinal_encoder.transform(X_valid[object_cols])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## One-Hot Encoding","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\n\n# Apply one-hot encoder to each column with categorical data\nOH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\nOH_cols_train = pd.DataFrame(OH_encoder.fit_transform(X_train[object_cols]))\nOH_cols_valid = pd.DataFrame(OH_encoder.transform(X_valid[object_cols]))\n\n# One-hot encoding removed index; put it back\nOH_cols_train.index = X_train.index\nOH_cols_valid.index = X_valid.index\n\n# Remove categorical columns (will replace with one-hot encoding)\nnum_X_train = X_train.drop(object_cols, axis=1)\nnum_X_valid = X_valid.drop(object_cols, axis=1)\n\n# Add one-hot encoded columns to numerical features\nOH_X_train = pd.concat([num_X_train, OH_cols_train], axis=1)\nOH_X_valid = pd.concat([num_X_valid, OH_cols_valid], axis=1)\n\n# Ensure all columns have string type\nOH_X_train.columns = OH_X_train.columns.astype(str)\nOH_X_valid.columns = OH_X_valid.columns.astype(str)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Checking for cardinality","metadata":{}},{"cell_type":"code","source":"# Get number of unique entries in each column with categorical data\nobject_nunique = list(map(lambda col: X_train[col].nunique(), object_cols))\nd = dict(zip(object_cols, object_nunique))\n\n# Print number of unique entries by column, in ascending order\nsorted(d.items(), key=lambda x: x[1])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fit the model to the training data\nmy_model.fit(X, y)\n\n# Generate test predictions\npreds_test = my_model.predict(X_test)\n\n# Save predictions in format used for competition scoring\noutput = pd.DataFrame({'Id': X_test.index,\n                       'SalePrice': preds_test})\noutput.to_csv('submission.csv', index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Pipelines**","metadata":{}},{"cell_type":"markdown","source":"### **Loading of Data**","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# Read the data\nX = pd.read_csv('../input/train.csv', index_col='Id')\nX_test_full = pd.read_csv('../input/test.csv', index_col='Id')\n\n# Remove rows with missing target, separate target from predictors\nX.dropna(axis=0, subset=['SalePrice'], inplace=True)\ny = X.SalePrice              \nX.drop(['SalePrice'], axis=1, inplace=True)\n\n# Break off validation set from training data\nX_train_full, X_valid_full, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,\n                                                                random_state=0)\n\n# \"Cardinality\" means the number of unique values in a column\n# Select categorical columns with relatively low cardinality (convenient but arbitrary)\nlow_cardinality_cols = [cname for cname in X_train_full.columns if X_train_full[cname].nunique() < 10 and \n                        X_train_full[cname].dtype == \"object\"]\n\n# Select numeric columns\nnumeric_cols = [cname for cname in X_train_full.columns if X_train_full[cname].dtype in ['int64', 'float64']]\n\n# Keep selected columns only\nmy_cols = low_cardinality_cols + numeric_cols\nX_train = X_train_full[my_cols].copy()\nX_valid = X_valid_full[my_cols].copy()\nX_test = X_test_full[my_cols].copy()\n\n# One-hot encode the data (to shorten the code, we use pandas)\nX_train = pd.get_dummies(X_train)\nX_valid = pd.get_dummies(X_valid)\nX_test = pd.get_dummies(X_test)\nX_train, X_valid = X_train.align(X_valid, join='left', axis=1)\nX_train, X_test = X_train.align(X_test, join='left', axis=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Step 1: Define Preprocessing Steps**","metadata":{}},{"cell_type":"code","source":"from sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\n\n# Preprocessing for numerical data\nnumerical_transformer = SimpleImputer(strategy='constant')\n\n# Preprocessing for categorical data\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\n# Bundle preprocessing for numerical and categorical data\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer, numerical_cols),\n        ('cat', categorical_transformer, categorical_cols)\n    ])\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Step 2: Define the Model**","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\n\nmodel = RandomForestRegressor(n_estimators=100, random_state=0)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Step 3: Create and Evaluate the Pipeline**","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import mean_absolute_error\n\n# Bundle preprocessing and modeling code in a pipeline\nmy_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n                              ('model', model)\n                             ])\n\n# Preprocessing of training data, fit model \nmy_pipeline.fit(X_train, y_train)\n\n# Preprocessing of validation data, get predictions\npreds = my_pipeline.predict(X_valid)\n\n# Evaluate the model\nscore = mean_absolute_error(y_valid, preds)\nprint('MAE:', score)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Step 4: Run prediction on Test Data and generate submission.**","metadata":{}},{"cell_type":"code","source":"preds_test = my_pipeline.predict(X_test)\n\noutput = pd.DataFrame({'Id': X_test.index,\n                       'SalePrice': preds_test})\noutput.to_csv('submission.csv', index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\n\n# Multiply by -1 since sklearn calculates *negative* MAE\nscores = -1 * cross_val_score(my_pipeline, X, y,\n                              cv=5,\n                              scoring='neg_mean_absolute_error')\n\nprint(\"MAE scores:\\n\", scores)\nprint(\"Average MAE score (across experiments):\")\nprint(scores.mean())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Building pipeline with choosing best n_estimator","metadata":{}},{"cell_type":"code","source":"def get_score(n_estimators):\n    \"\"\"Return the average MAE over 3 CV folds of random forest model.\n    \n    Keyword argument:\n    n_estimators -- the number of trees in the forest\n    \"\"\"\n\n   \n    my_pipeline = Pipeline(steps=[\n    ('preprocessor', SimpleImputer()),\n    ('model',  RandomForestRegressor(n_estimators, random_state=0))\n])\n    scores = -1 * cross_val_score(my_pipeline, X, y,\n                              cv=3,\n                              scoring='neg_mean_absolute_error')\n    return(scores.mean())\n\nresults = {}\nfor i in range(1,9):\n    results[50*i] = get_score(50*i)\n    \n#     Visualizing best n_estimator\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nplt.plot(list(results.keys()), list(results.values()))\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **XGBoost (Xtreme Gradient Boosting)**","metadata":{}},{"cell_type":"code","source":"from xgboost import XGBRegressor\n\nmy_model = XGBRegressor()\nmy_model.fit(X_train, y_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import mean_absolute_error\n\npredictions = my_model.predict(X_valid)\nprint(\"Mean Absolute Error: \" + str(mean_absolute_error(predictions, y_valid)))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"my_model = XGBRegressor(n_estimators=500)\nmy_model.fit(X_train, y_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"my_model_3 = XGBRegressor(n_estimators=100, learning_rate=1)\n\n# Fit the model\nmy_model_3.fit(X_train, y_train,\n              early_stopping_rounds=1,\n              eval_set=[(X_valid, y_valid)],\n              verbose=False)\n\n# Get predictions\npredictions_3 = my_model_3.predict(X_valid)\n\n# Calculate MAE\nmae_3 = mean_absolute_error(predictions_3, y_valid)\n\n# Uncomment to print MAE\nprint(\"Mean Absolute Error:\" , mae_3)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Clean XGBoost Pipeline**","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# Read the data\nX_full = pd.read_csv('../input/train.csv', index_col='Id')\nX_test_full = pd.read_csv('../input/test.csv', index_col='Id')\n\n# Remove rows with missing target, separate target from predictors\nX_full.dropna(axis=0, subset=['SalePrice'], inplace=True)\ny = X_full.SalePrice\nX_full.drop(['SalePrice'], axis=1, inplace=True)\n\n# Break off validation set from training data\nX_train_full, X_valid_full, y_train, y_valid = train_test_split(X_full, y, \n                                                                train_size=0.8, test_size=0.2,\n                                                                random_state=0)\n\n# \"Cardinality\" means the number of unique values in a column\n# Select categorical columns with relatively low cardinality (convenient but arbitrary)\ncategorical_cols = [cname for cname in X_train_full.columns if\n                    X_train_full[cname].nunique() < 10 and \n                    X_train_full[cname].dtype == \"object\"]\n\n# Select numerical columns\nnumerical_cols = [cname for cname in X_train_full.columns if \n                X_train_full[cname].dtype in ['int64', 'float64']]\n\n# Keep selected columns only\nmy_cols = categorical_cols + numerical_cols\nX_train = X_train_full[my_cols].copy()\nX_valid = X_valid_full[my_cols].copy()\nX_test = X_test_full[my_cols].copy()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom xgboost import XGBRegressor","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Preprocessing for numerical data\nnumerical_transformer = SimpleImputer(strategy='median')\n\n# Preprocessing for categorical data\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\n# Bundle preprocessing for numerical and categorical data\npreprocessor = ColumnTransformer(transformers=[\n    ('num', numerical_transformer, numerical_cols),\n    ('cat', categorical_transformer, categorical_cols)\n])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define and tune the model parameters\nmodel = XGBRegressor(n_estimators=10000,\n                     learning_rate=0.01,\n                     random_state=0,\n                    booster='gbtree',\n                    gamma=1e8,\n                    max_depth=6,\n                    objective=\"reg:squarederror\",\n                    seed=0)\n\n# Bundle preprocessing and modeling code in a pipeline\nXGB_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n                              ('model', model)\n                             ])\n\npreprocessor.fit_transform(X_train)\nX_valid_transformed = preprocessor.transform(X_valid)\nfit_params = {\"model__eval_set\": [(X_valid_transformed, y_valid)], \n              \"model__early_stopping_rounds\": 5,\n              \"model__verbose\": False}\n\n# Preprocessing of training data, fit model \nXGB_pipeline.fit(X_train, y_train, **fit_params)\n\n# Preprocessing of validation data, get predictions\npreds = XGB_pipeline.predict(X_valid)\n\n# Evaluate the model\nprint('MAE:', mean_absolute_error(y_valid, preds))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Preprocessing of test data, fit model\npreds_test = XGB_pipeline.predict(X_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save test predictions to file\noutput = pd.DataFrame({'Id': X_test.index,\n                       'SalePrice': preds_test})\noutput.to_csv('submission.csv', index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Data Visualization**","metadata":{}},{"cell_type":"code","source":"import pandas as pd\npd.plotting.register_matplotlib_converters()\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\nspotify_filepath = \"../input/spotify.csv\"\n\nspotify_data = pd.read_csv(spotify_filepath, index_col=\"Date\", parse_dates=True)\n\n\nplt.figure(figsize=(16,6))\nplt.title(\"Daily Global Streams of Popular Songs in 2017-2018\")\nsns.lineplot(data=spotify_data)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Line chart**","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(14,6))\n\n# Add title\nplt.title(\"Daily Global Streams of Popular Songs in 2017-2018\")\n\n# Line chart showing daily global streams of 'Shape of You'\nsns.lineplot(data=spotify_data['Shape of You'], label=\"Shape of You\")\n\n# Line chart showing daily global streams of 'Despacito'\nsns.lineplot(data=spotify_data['Despacito'], label=\"Despacito\")\n\n# Add label for horizontal axis\nplt.xlabel(\"Date\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Bar chart**","metadata":{}},{"cell_type":"code","source":"# Read the file into a variable flight_data\nflight_data = pd.read_csv(flight_filepath, index_col=\"Month\")\n\n# Set the width and height of the figure\nplt.figure(figsize=(10,6))\n\n# Add title\nplt.title(\"Average Arrival Delay for Spirit Airlines Flights, by Month\")\n\n# Bar chart showing average arrival delay for Spirit Airlines flights by month\nsns.barplot(x=flight_data.index, y=flight_data['NK'])\n\n# Add label for vertical axis\nplt.ylabel(\"Arrival delay (in minutes)\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Heat map**","metadata":{}},{"cell_type":"code","source":"# Set the width and height of the figure\nplt.figure(figsize=(14,7))\n\n# Add title\nplt.title(\"Average Arrival Delay for Each Airline, by Month\")\n\n# Heatmap showing average arrival delay for each airline by month\nsns.heatmap(data=flight_data, annot=True, cmap=\"coolwarm\")\n\n# Add label for horizontal axis\nplt.xlabel(\"Airline\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Scatter plot**","metadata":{}},{"cell_type":"code","source":"sns.scatterplot(x=insurance_data['bmi'], y=insurance_data['charges'])\nsns.regplot(x=insurance_data['bmi'], y=insurance_data['charges'])\nsns.lmplot(x=\"bmi\", y=\"charges\", hue=\"smoker\", data=insurance_data)\nsns.scatterplot(x=insurance_data['bmi'], y=insurance_data['charges'], hue=insurance_data['smoker'])\n\nsns.swarmplot(x=insurance_data['smoker'],\n              y=insurance_data['charges'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Distributions**","metadata":{}},{"cell_type":"code","source":"sns.histplot(iris_data['Petal Length (cm)'])\nsns.kdeplot(data=iris_data['Petal Length (cm)'], shade=True)\nsns.jointplot(x=iris_data['Petal Length (cm)'], y=iris_data['Sepal Width (cm)'], kind=\"kde\")\n\n\n\n# Histograms for each species\nsns.histplot(data=iris_data, x='Petal Length (cm)', hue='Species')\n\n# Add title\nplt.title(\"Histogram of Petal Lengths, by Species\")\n\n# KDE plots for each species\nsns.kdeplot(data=iris_data, x='Petal Length (cm)', hue='Species', shade=True)\n\n# Add title\nplt.title(\"Distribution of Petal Lengths, by Species\")\n\n\n\n# Change the style of the figure to the \"dark\" theme\nsns.set_style(\"dark\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Feature Engineering**","metadata":{}},{"cell_type":"markdown","source":"### **Baseline scoring**","metadata":{}},{"cell_type":"code","source":"X = df.copy()\ny = X.pop(\"CompressiveStrength\")\n\n# Train and score baseline model\nbaseline = RandomForestRegressor(criterion=\"absolute_error\", random_state=0)\nbaseline_score = cross_val_score(\n    baseline, X, y, cv=5, scoring=\"neg_mean_absolute_error\"\n)\nbaseline_score = -1 * baseline_score.mean()\n\nprint(f\"MAE Baseline Score: {baseline_score:.4}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Creating of synthetic features (Ratios)**","metadata":{}},{"cell_type":"code","source":"X = df.copy()\ny = X.pop(\"CompressiveStrength\")\n\n# Create synthetic features\nX[\"FCRatio\"] = X[\"FineAggregate\"] / X[\"CoarseAggregate\"]\nX[\"AggCmtRatio\"] = (X[\"CoarseAggregate\"] + X[\"FineAggregate\"]) / X[\"Cement\"]\nX[\"WtrCmtRatio\"] = X[\"Water\"] / X[\"Cement\"]\n\n# Train and score model on dataset with additional ratio features\nmodel = RandomForestRegressor(criterion=\"absolute_error\", random_state=0)\nscore = cross_val_score(\n    model, X, y, cv=5, scoring=\"neg_mean_absolute_error\"\n)\nscore = -1 * score.mean()\n\nprint(f\"MAE Score with Ratio Features: {score:.4}\")","metadata":{},"execution_count":null,"outputs":[]}]}